# StrongSORT with OSNet for YoloV5 and YoloV7 (Counter)





<div align="center">
<p>
<img src="MOT16_eval/track_pedestrians.gif" width="300"/>  <img src="MOT16_eval/track_all.gif" width="300"/> 
</p>
<br>
# Official YOLOv5
<div>
<a href="https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch/actions"><img src="https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch/workflows/CI%20CPU%20testing/badge.svg" alt="CI CPU testing"></a>
<br>  
<a href="https://colab.research.google.com/drive/18nIqkBr68TkK8dHdarxTco6svHUJGggY?usp=sharing"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>
 
</div>
# Official YOLOv7

Implementation of paper - [YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors](https://arxiv.org/abs/2207.02696)

[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/yolov7-trainable-bag-of-freebies-sets-new/real-time-object-detection-on-coco)](https://paperswithcode.com/sota/real-time-object-detection-on-coco?p=yolov7-trainable-bag-of-freebies-sets-new)
[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/akhaliq/yolov7)
<a href="https://colab.research.google.com/gist/AlexeyAB/b769f5795e65fdab80086f6cb7940dae/yolov7detection.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>
[![arxiv.org](http://img.shields.io/badge/cs.CV-arXiv%3A2207.02696-B31B1B.svg)](https://arxiv.org/abs/2207.02696)
</div>


## Introduction

This repository contains a highly configurable two-stage-tracker that adjusts to different deployment scenarios. The detections generated by [YOLOv5](https://github.com/ultralytics/yolov5) and [YOLOv7](https://github.com/WongKinYiu/yolov7), a family of object detection architectures and models pretrained on the COCO dataset, are passed to [StrongSORT](https://github.com/dyhBUPT/StrongSORT)[](https://arxiv.org/pdf/2202.13514.pdf) which combines motion and appearance information based on [OSNet](https://github.com/KaiyangZhou/deep-person-reid)[](https://arxiv.org/abs/1905.00953) in order to tracks the objects. It can track any object that your Yolov5 model was trained to detect.


## Before you run the tracker

1. Clone the repository recursively:

`git clone --recurse-submodules https://github.com/bharath5673/StrongSORT-YOLO.git `

If you already cloned and forgot to use `--recurse-submodules` you can run `git submodule update --init`

2. Make sure that you fulfill all the requirements: Python 3.8 or later with all [requirements.txt](https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch/blob/master/requirements.txt) dependencies installed, including torch>=1.7. To install, run:

`pip install -r requirements.txt`


## Tracking sources

Tracking can be run on most video formats


## Select object detectors and ReID model

### Yolov5

There is a clear trade-off between model inference speed and accuracy. In order to make it possible to fulfill your inference speed/accuracy needs
you can select a Yolov5 family model for automatic download

```bash


$ python track_v5.py --source 0 --yolo-weights weights/yolov5n.pt --img 640
                                            yolov5s.pt
                                            yolov5m.pt
                                            yolov5l.pt 
                                            yolov5x.pt --img 1280
                                            ...
```

### Yolov7

There is a clear trade-off between model inference speed and accuracy. In order to make it possible to fulfill your inference speed/accuracy needs
you can select a Yolov5 family model for automatic download

```bash


$ python track_v7.py --source 0 --yolo-weights weights/yolov7-tiny.pt --img 640
                                            yolov7.pt
                                            yolov7x.pt 
                                            yolov7-w6.pt 
                                            yolov7-e6.pt 
                                            yolov7-d6.pt 
                                            yolov7-e6e.pt
                                            ...
```


### StrongSORT

The above applies to StrongSORT models as well. Choose a ReID model based on your needs from this ReID [model zoo](https://kaiyangzhou.github.io/deep-person-reid/MODEL_ZOO)

```bash


$ python track_v*.py --source 0 --strong-sort-weights osnet_x0_25_market1501.pt
                                                   osnet_x0_5_market1501.pt
                                                   osnet_x0_75_msmt17.pt
                                                   osnet_x1_0_msmt17.pt
                                                   ...
```

## Filter tracked classes

By default the tracker tracks all MS COCO classes.

If you only want to track persons I recommend you to get [these weights](https://drive.google.com/file/d/1gglIwqxaH2iTvy6lZlXuAcMpd_U0GCUb/view?usp=sharing) for increased performance

```bash
python track_v*.py --source 0 --yolo-weights weights/v*.pt --classes 0  # tracks persons, only
```

If you want to track a subset of the MS COCO classes, add their corresponding index after the classes flag

```bash
python track_v*.py --source 0 --yolo-weights  weights/v*.pt --classes 16 17  # tracks cats and dogs, only
```

### Counter

![counter](demo.gif)

#### get realtime counts of every tracking objects without any rois or any line interctions

```bash

$ python track_v*.py --source test.mp4 -yolo-weights weights/v*.pt --save-txt --count --show-vid

```


### Draw Object Trajectory <img src="https://media0.giphy.com/media/J19OSJKmqCyP7Mfjt1/giphy.gif" width="80" height="30" />

```bash

$ python track_v*.py --source test.mp4 -yolo-weights weights/v*.pt --save-txt --count --show-vid --draw

```


[Here](https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/) is a list of all the possible objects that a Yolov5 model trained on MS COCO can detect. Notice that the indexing for the classes in this repo starts at zero.


## MOT compliant results

Can be saved to your experiment folder `runs/track/<yolo_model>_<deep_sort_model>/` by 

```bash
python track_v*.py --source ... --save-txt
```


## Cite

If you find this project useful in your research, please consider cite:

```latex
@misc{yolov5-strongsort-osnet-2022,
    title={Real-time multi-camera multi-object tracker using YOLOv5 and StrongSORT with OSNet},
    author={Mikel BrostrÃ¶m},
    howpublished = {\url{https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet}},
    year={2022}
}

@article{wang2022yolov7,
  title={{YOLOv7}: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},
  author={Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
  journal={arXiv preprint arXiv:2207.02696},
  year={2022}
}
```
## Customization 
### Research on getting lost tracks when they reapear in a camera
```bash
$ python track_v5x.py -h
$ python track_v5x.py --source 0 --yolo-weights weights/yolov7-tiny.pt --img 640 --compare --save-tracklets --show-vid --hide-conf --classes 0 
```
In the above code, we are running yolov5 based detector with reidentification and strongsort tracker. Please select
an appropriate yolov5 detector for your application. It is recommended that yolov5 model is trained on a large dataset
to detect objects in a video file correctly. Set appropriate image size, recommended size is 416 or 640, but you can try different as per the need. Moreover, ```compare``` commandline flag is set to compare appearance features throughtout the video feed to compare appearance features for lost tracks identification and matching. And, ```save_tracklets``` commandline flag is set to save the appearance data to the file, such as a csv file. 

### code explanation
In the ```track_v5x.py``` file, we first create a csv file to store data at line 139 - 140 with the following code script.

```python
tracklets_data = "/home/facit/faizan/tracker/StrongSORT-YOLO/tracklets.csv"
colnames=['id', 'cls', 'frame', 'bbox', 'appearance']
```
In the first line we specify the path to the file and in the subsquent line we add columns names mentioned in the list
above. For instance, ```id``` stores id of the track which is a unique id of an object in a frame, we also add ```cls``` class of the object, it is always person so this is not gonna change. Next, we add ```frame``` number to specify the frame index for which the track data corresponds. For tracking by detection method, it's very important to detect the object first, for that we also stored ```bbox``` bounding boxes of objects which are useful for localizing the object in a frame. Finally, we need appearance embeddings for accurate data matching, we must store it in the file. We are storing it as ```appearance``` features. Which is n-dimensional vector. 

Then,
from line 255 - 306 we perform some important processing and matcing steps as mentioned below:
```python          
                if outputs[i] is not None:
                    tracklets = []
                    if len(outputs[i]) > 0:
                        for j, (output, conf) in enumerate(zip(outputs[i], confs)):
                            tracklet = {}
                            bboxes = output["bbox"]
                            tracklet["bbox"] = bboxes
                            id = output["id"]
                            tracklet["id"] = id
                            cls = output["cls"]
                            tracklet["cls"] = cls
                            output["frame"] = frame_idx
                            tracklet["frame"] = frame_idx
                            appearance = output["appearance"]
                            track = output["track"]
                            if type(appearance) != list and type(appearance) == np.ndarray:
                                tracklet["appearance"] = str(appearance.flatten().tolist())
                            else:
                                tracklet["appearance"] = appearance
                            bbox_left, bbox_top, bbox_right, bbox_bottom = bboxes

                            tracklets.append(tracklet)
                            frame_ids.append(id)
                            if compare and save_tracklets:
                                 if os.path.exists(tracklets_data):
                                    tracks_df = pd.read_csv(tracklets_data, names=colnames, header=None)
                                    if not tracks_df.empty and frame_idx >= 3:
                                        ids = unique_ids(tracks_df)
                                        if id not in ids:
                                            #-----------------------------------------------------------
                                            # old code: using the latest appearance
                                            # previous_tracklets = latest_appearance(tracks_df, ids)
                                            # assigned_id = compare_appearance(previous_tracklets, tracklet)
                                            # if assigned_id is not None:
                                            #     track.track_id = assigned_id

                                            #-------------------------------------------------------------

                                            #code using all previous appearance features
                                            tic = time.time()
                                            all_previous_appearances = calculate_matrix_from_df(tracks_df)
                                            score, index = find_match(all_previous_appearances, appearance)
                                            assigned_id = get_id(tracks_df, index)
                                            toc = time.time()
                                            print(f'Feature matching duration:  {toc - tic}')
                                            if assigned_id is not None:
                                                if assigned_id in frame_ids:
                                                    assigned_id = id
                                                track.track_id = assigned_id

```
from line 197 to 210, we are looping over each  track in a frame index, and unpacking the dictionary data returned
from the strongsort class method named update and storing them in local variables for later use. From 211 - 218, we are checking data type of appearance features and storing tracklet data and append its corresponding id for storing in a csv file in each frame index.

From line 219 - 244 above, we are checking if the user specified ```--compare``` and ```save-tracklets``` command line arguments, if they specified; then, we are checking if the file exists on the path or not, it it does, then we move on to the next step. The next step is to read the file and use it if its not empty and frame index is greater than 3. Its important to check for the new ids in a frame. If a new id appears in a particular frame then we must check if it corresponds to an old id disappeared somewhere in the old frame index. If it does then we need to assign it with the an old id, it is a new id then we need to give it a random number. 

We then check this unique id should not be in old ids. The commented code from 227 to  234 should be used to compare new id with the latest appearance features and not with all data. If this is the case; then, please uncomment this. At line 236, we calculate the appearance matrix from the start frame to current frame using csv file. Then, at 237 line we use this matrix and the current appearance to find matches, if the match exists with old features it will return match score and index. We are calculating this matching using Cosine distance equation. At line 238, we are finding the id that should be assigned to a new track.  And finally, we are assigning this id to dynamic tracker id to make it consistent throughout the track movment. 


## Acknowledgements

<details><summary> <b>Expand</b> </summary>

* [https://github.com/AlexeyAB/darknet](https://github.com/AlexeyAB/darknet)
* [https://github.com/WongKinYiu/yolor](https://github.com/WongKinYiu/yolor)
* [https://github.com/WongKinYiu/PyTorch_YOLOv4](https://github.com/WongKinYiu/PyTorch_YOLOv4)
* [https://github.com/WongKinYiu/ScaledYOLOv4](https://github.com/WongKinYiu/ScaledYOLOv4)
* [https://github.com/Megvii-BaseDetection/YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)
* [https://github.com/ultralytics/yolov3](https://github.com/ultralytics/yolov3)
* [https://github.com/ultralytics/yolov5](https://github.com/ultralytics/yolov5)
* [https://github.com/DingXiaoH/RepVGG](https://github.com/DingXiaoH/RepVGG)
* [https://github.com/JUGGHM/OREPA_CVPR2022](https://github.com/JUGGHM/OREPA_CVPR2022)
* [https://github.com/TexasInstruments/edgeai-yolov5/tree/yolo-pose](https://github.com/TexasInstruments/edgeai-yolov5/tree/yolo-pose)

</details>

